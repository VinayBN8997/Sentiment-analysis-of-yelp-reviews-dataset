{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of YELP reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This data was obtained for the kaggle competition: https://www.kaggle.com/c/sentclf\n",
    "Since it was private competition, we are using the train data only and then train-test split it\n",
    "All rights are own by YELP reviews dataset.\n",
    "Please refer acknowledgement of the kaggle page to more details on restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinaybn/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great mobile app with nice reward program. Mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Really fast and polite. Definitely recommend. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This place is always amazing, friendly staff a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>We did a Wine 101 class on a Friday night. Coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I am rounding up because I think this place ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0     1  Great mobile app with nice reward program. Mak...\n",
       "1     2  Really fast and polite. Definitely recommend. ...\n",
       "2     2  This place is always amazing, friendly staff a...\n",
       "3     1  We did a Wine 101 class on a Friday night. Coo...\n",
       "4     1  I am rounding up because I think this place ma..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400001 entries, 0 to 400000\n",
      "Data columns (total 2 columns):\n",
      "label    400001 non-null object\n",
      "text     400000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To download stopwords:\n",
    "Go to command prompt or terminal and run the below commands\n",
    "->import nltk \n",
    "->nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))\n",
    "exclude = set(string.punctuation)\n",
    "translate = Translator()\n",
    "sno = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sent):\n",
    "    sent = sent.lower()\n",
    "    temp = ''\n",
    "    for ch in sent:\n",
    "        if ch not in exclude:\n",
    "            temp +=ch\n",
    "        else:\n",
    "            temp+=''\n",
    "    arr = word_tokenize(temp)\n",
    "    sent = \"\"\n",
    "    for i in arr:\n",
    "        if i!='' and i!=' ' and (i not in stopwords):\n",
    "            sent +=i+' '\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = df.text.values\n",
    "inp_processed = []\n",
    "err = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(inp))):\n",
    "    #sent = unidecode(inp[i].decode('utf-8'))\n",
    "    sent = inp[i]\n",
    "    if detect(sent) != 'en':\n",
    "        try:\n",
    "            sent = translate.translate(sent).text\n",
    "        except:\n",
    "            err +=1\n",
    "    sent = get_words(sent)\n",
    "    stemmed = ''\n",
    "    for j in sent.split(\" \"):\n",
    "        stemmed+= sno.stem(j)+\" \"\n",
    "    df.loc[i,'text'] = stemmed.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"stemmed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "from sklearn import utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stemmed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>great mobil app nice reward program make reser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>realli fast polit definit recommend also clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>place alway amaz friend staff great deal produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wine 101 class friday night cool spot downstai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>round think place may potenti coupl thing tri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  great mobil app nice reward program make reser...\n",
       "1      2  realli fast polit definit recommend also clean...\n",
       "2      2  place alway amaz friend staff great deal produ...\n",
       "3      1  wine 101 class friday night cool spot downstai...\n",
       "4      1  round think place may potenti coupl thing tri ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.text\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Negative% Moderate% Positive%\n",
      "385613 20.435773690202353 16.961046437749765 62.60317987204789\n",
      "3935 21.296060991105463 17.941550190597205 60.76238881829733\n",
      "3935 21.092757306226176 16.16264294790343 62.74459974587039\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Negative% Moderate% Positive%\")\n",
    "print(len(x_train),(len(x_train[y_train == 0]) / (len(x_train)))*100,(len(x_train[y_train == 1]) / (len(x_train)))*100,(len(x_train[y_train == 2]) / (len(x_train)))*100)\n",
    "print(len(x_validation),(len(x_validation[y_validation == 0]) / (len(x_validation)))*100,(len(x_validation[y_validation == 1]) / (len(x_validation)))*100,(len(x_validation[y_validation == 2]) / (len(x_validation)*1.))*100)\n",
    "print(len(x_test),(len(x_test[y_test == 0]) / (len(x_test)))*100,(len(x_test[y_test == 1]) / (len(x_test)))*100,(len(x_test[y_test == 2]) / (len(x_test))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████▋   | 262284/393483 [00:00<00:00, 2604474.63it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 2560979.40it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sg=1, size=300, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 68%|██████▊   | 266290/393483 [00:00<00:00, 2648524.24it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 2633641.62it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 68%|██████▊   | 267420/393483 [00:00<00:00, 2657815.76it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 2634486.63it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 55%|█████▍    | 214727/393483 [00:00<00:00, 2135267.75it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 2067232.31it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|███       | 119836/393483 [00:00<00:00, 1188778.47it/s]\u001b[A\n",
      " 61%|██████    | 240091/393483 [00:00<00:00, 1193975.72it/s]\u001b[A\n",
      " 94%|█████████▍| 370581/393483 [00:00<00:00, 1232167.97it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1242826.11it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 35%|███▍      | 135776/393483 [00:00<00:00, 1347343.74it/s]\u001b[A\n",
      " 72%|███████▏  | 281627/393483 [00:00<00:00, 1402442.05it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1427722.57it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 128844/393483 [00:00<00:00, 1277497.69it/s]\u001b[A\n",
      " 63%|██████▎   | 246049/393483 [00:00<00:00, 1222933.59it/s]\u001b[A\n",
      " 96%|█████████▌| 376897/393483 [00:00<00:00, 1251071.84it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1249433.97it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 35%|███▍      | 137640/393483 [00:00<00:00, 1366273.89it/s]\u001b[A\n",
      " 71%|███████   | 280184/393483 [00:00<00:00, 1395370.53it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1467705.01it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|███▎      | 132317/393483 [00:00<00:00, 1312665.06it/s]\u001b[A\n",
      " 68%|██████▊   | 266923/393483 [00:00<00:00, 1329454.47it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1372812.81it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 32%|███▏      | 125761/393483 [00:00<00:00, 1244960.64it/s]\u001b[A\n",
      " 62%|██████▏   | 242232/393483 [00:00<00:00, 1203950.60it/s]\u001b[A\n",
      " 96%|█████████▌| 376337/393483 [00:00<00:00, 1250784.90it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1252500.48it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 32%|███▏      | 124723/393483 [00:00<00:00, 1237284.07it/s]\u001b[A\n",
      " 62%|██████▏   | 244469/393483 [00:00<00:00, 1216725.67it/s]\u001b[A\n",
      " 94%|█████████▍| 371713/393483 [00:00<00:00, 1234239.15it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1205031.70it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 33s, sys: 5.59 s, total: 1h 9min 38s\n",
      "Wall time: 9min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    model.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('w2v_model_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 48%|████▊     | 187649/393483 [00:00<00:00, 1866448.22it/s]\u001b[A\n",
      " 89%|████████▉ | 350564/393483 [00:00<00:00, 1747185.59it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1796826.92it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sg=0, size=300, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▎     | 172023/393483 [00:00<00:00, 1710160.60it/s]\u001b[A\n",
      " 91%|█████████ | 357596/393483 [00:00<00:00, 1782567.24it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1822275.26it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 32%|███▏      | 124338/393483 [00:00<00:00, 1230356.88it/s]\u001b[A\n",
      " 62%|██████▏   | 243338/393483 [00:00<00:00, 1211500.62it/s]\u001b[A\n",
      " 94%|█████████▍| 369796/393483 [00:00<00:00, 1228003.23it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1217941.40it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|███       | 119772/393483 [00:00<00:00, 1187452.70it/s]\u001b[A\n",
      " 62%|██████▏   | 245873/393483 [00:00<00:00, 1224835.42it/s]\u001b[A\n",
      " 99%|█████████▉| 389763/393483 [00:00<00:00, 1295976.17it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1290395.39it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|███▎      | 132239/393483 [00:00<00:00, 1311785.76it/s]\u001b[A\n",
      " 63%|██████▎   | 249792/393483 [00:00<00:00, 1243892.85it/s]\u001b[A\n",
      " 95%|█████████▍| 372897/393483 [00:00<00:00, 1238946.34it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1228968.95it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▉       | 114639/393483 [00:00<00:00, 1136079.35it/s]\u001b[A\n",
      " 57%|█████▋    | 223296/393483 [00:00<00:00, 1110017.01it/s]\u001b[A\n",
      " 85%|████████▍ | 334243/393483 [00:00<00:00, 1104443.68it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1001655.88it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|███▍      | 135390/393483 [00:00<00:00, 1343713.63it/s]\u001b[A\n",
      " 72%|███████▏  | 282744/393483 [00:00<00:00, 1407105.67it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1481031.38it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▏     | 164975/393483 [00:00<00:00, 1638136.24it/s]\u001b[A\n",
      " 82%|████████▏ | 321312/393483 [00:00<00:00, 1598990.32it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1667310.52it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 39%|███▉      | 155235/393483 [00:00<00:00, 1539541.24it/s]\u001b[A\n",
      " 79%|███████▊  | 309506/393483 [00:00<00:00, 1542357.56it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1631765.61it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 41%|████▏     | 163100/393483 [00:00<00:00, 1620270.06it/s]\u001b[A\n",
      " 80%|████████  | 315671/393483 [00:00<00:00, 1572690.87it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1658219.09it/s]\u001b[A\n",
      "  0%|          | 0/393483 [00:00<?, ?it/s]\u001b[A\n",
      " 36%|███▌      | 142534/393483 [00:00<00:00, 1413291.84it/s]\u001b[A\n",
      " 70%|███████   | 276366/393483 [00:00<00:00, 1375374.67it/s]\u001b[A\n",
      "100%|██████████| 393483/393483 [00:00<00:00, 1424304.60it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 18s, sys: 5.03 s, total: 27min 23s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    model.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('w2v_model_cbow.word2vec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW means continuous bag of words. SG means Skip Gram. \n",
    "With a corpus, CBOW model predicts the current word from a window of surrounding context words, while Skip-gram model predicts surrounding context words given the current word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we have the following sentence: \"I love dogs\". CBOW model tries to predict the word \"love\" when given \"I\", \"dogs\" as inputs, on the other hand, Skip-gram model tries to predict \"I\", \"dogs\" when given the word \"love\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  gensim.models import KeyedVectors\n",
    "model_sg = KeyedVectors.load('w2v_model_sg.word2vec')\n",
    "model_cbow = KeyedVectors.load('w2v_model_cbow.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57291"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_sg.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (385613, 80)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_seq = pad_sequences(sequences, maxlen=80)\n",
    "print('Shape of data tensor:', x_train_seq.shape)\n",
    "partial_data = pd.DataFrame({\"text_vectors\":np.ndarray.tolist(x_train_seq),'label':y_train},dtype = 'object')\n",
    "partial_data.to_pickle(\"train_tokanized.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3935, 80)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x_validation)\n",
    "x_validation_seq = pad_sequences(sequences, maxlen=80)\n",
    "print('Shape of data tensor:', x_validation_seq.shape)\n",
    "partial_data = pd.DataFrame({\"text_vectors\":np.ndarray.tolist(x_validation_seq),'label':y_validation},dtype = 'object')\n",
    "partial_data.to_pickle(\"validation_tokanized.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3935, 80)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences, maxlen=80)\n",
    "print('Shape of data tensor:', x_test_seq.shape)\n",
    "partial_data = pd.DataFrame({\"text_vectors\":np.ndarray.tolist(x_test_seq),'label':y_test},dtype = 'object')\n",
    "partial_data.to_pickle(\"test_tokanized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57291 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_cbow.wv[w],model_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "num_words  = 30000\n",
    "embedding_matrix = np.zeros((num_words, 600))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model of Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train,num_classes = 3)\n",
    "y_validation = to_categorical(y_validation,num_classes = 3)\n",
    "y_test = to_categorical(y_test,num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "seed = 7\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Flatten , Conv2D, MaxPooling2D, Reshape, ConvLSTM2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = Sequential()\n",
    "e = Embedding(30000, 600,  input_length=80, trainable=True) #weights=[embedding_matrix]\n",
    "model.add(e)\n",
    "model.add(Reshape((1,80,600),input_shape=(100000,)))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same',strides = 2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same',strides = 2))              \n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 600)           18000000  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 80, 600)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 80, 16)         86416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 40, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 40, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 40, 16)         2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 20, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 20, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 963       \n",
      "=================================================================\n",
      "Total params: 18,089,699\n",
      "Trainable params: 18,089,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"CNN_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 385613 samples, validate on 3935 samples\n",
      "Epoch 1/3\n",
      "385613/385613 [==============================] - 1107s 3ms/step - loss: 0.3721 - acc: 0.8588 - val_loss: 0.3526 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85947, saving model to CNN_weights.h5\n",
      "Epoch 2/3\n",
      "385613/385613 [==============================] - 1133s 3ms/step - loss: 0.3345 - acc: 0.8749 - val_loss: 0.3288 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85947 to 0.87598, saving model to CNN_weights.h5\n",
      "Epoch 3/3\n",
      "385613/385613 [==============================] - 1126s 3ms/step - loss: 0.3238 - acc: 0.8802 - val_loss: 0.3333 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98c65e8ba8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_seq, y_train, validation_data=(x_validation_seq, y_validation), epochs=3, batch_size=8, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_CNN_model = load_model('CNN_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 600)           18000000  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 80, 600)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 80, 16)         86416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 40, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 40, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 40, 16)         2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 20, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 20, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 963       \n",
      "=================================================================\n",
      "Total params: 18,089,699\n",
      "Trainable params: 18,089,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(loaded_CNN_model, to_file='CNN_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3935/3935 [==============================] - 1s 146us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3144913269346521, 0.8815756033608996]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_CNN_model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = loaded_CNN_model.predict(x=x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 763,   37,   30],\n",
       "       [ 107,  373,  156],\n",
       "       [  47,   89, 2333]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(y_test,axis = 1),np.argmax(y_predict,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             precision    recall  f1-score   support\\n\\n          0       0.83      0.92      0.87       830\\n          1       0.75      0.59      0.66       636\\n          2       0.93      0.94      0.94      2469\\n\\navg / total       0.88      0.88      0.88      3935\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(np.argmax(y_test,axis = 1),np.argmax(y_predict,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
